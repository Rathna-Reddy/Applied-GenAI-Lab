{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e46cf62",
   "metadata": {},
   "source": [
    "### BUSINESS CHALLENGE\n",
    "Create a product that builds a Brochure for a company to be used for prospective clients, investors and potential clients\n",
    "We will be providing a company name and their primary website.\n",
    "Here we are doing in 2 ways:\n",
    "1. With OpenAI frontier model\n",
    "2. With Ollama local server (For this you should already have ollama up and running in your local) (you can follow the steps from [here](./Text_Summarization_Using_Llama.ipynb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d37080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from scraper import fetch_website_links, fetch_website_contents\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4e85e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "\n",
    "# MODEL = 'gpt-4o-mini'\n",
    "MODEL = 'gpt-5-nano'\n",
    "openai = OpenAI()\n",
    "\n",
    "# If you want to use Ollama, you can uncomment the following line and comment the above line and comment the above 2 lines for OpenAI\n",
    "# MODEL = \"llama3.2\"\n",
    "# openai = OpenAI(base_url = 'http://localhost:11434/v1', api_key='ollama')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is loaded, run below code if you are using Ollama\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d244ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = fetch_website_links('https://edwarddonner.com')\n",
    "# links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c460111",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = fetch_website_contents('https://edwarddonner.com')\n",
    "# print(contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30540c8",
   "metadata": {},
   "source": [
    "## First step: Have GPT-5-nano figure out which links are relevant\n",
    "\n",
    "### Use a call to gpt-5-nano to read the links on a webpage, and respond in structured JSON.  \n",
    "It should decide which links are relevant, and replace relative links such as \"/about\" with \"https://company.com/about\".  \n",
    "We will use \"one shot prompting\" in which we provide an example of how it should respond in the prompt.\n",
    "\n",
    "This is an excellent use case for an LLM, because it requires nuanced understanding. Imagine trying to code this without LLMs by parsing and analyzing the webpage - it would be very hard!\n",
    "\n",
    "Sidenote: there is a more advanced technique called \"Structured Outputs\" in which we require the model to respond according to a spec. I will cover this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313b779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_system_prompt = \"\"\"\n",
    "You are provided with a list of links found on a webpage.\n",
    "You are able to decide which of the links would be most relevant to include in a brochure about the company,\n",
    "such as links to an About page, or a Company page, or Careers/Jobs pages.\n",
    "You should respond in JSON as in this example:\n",
    "\n",
    "{\n",
    "    \"links\": [\n",
    "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
    "        {\"type\": \"careers page\", \"url\": \"https://another.full.url/careers\"}\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4679ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_user_prompt(url):\n",
    "    user_prompt = f\"\"\"\n",
    "Here is the list of links on the website {url} -\n",
    "Please decide which of these are relevant web links for a brochure about the company, \n",
    "respond with the full https URL in JSON format.\n",
    "Do not include Terms of Service, Privacy, email links.\n",
    "\n",
    "Links (some might be relative links):\n",
    "\n",
    "\"\"\"\n",
    "    links = fetch_website_links(url)\n",
    "    user_prompt += \"\\n\".join(links)\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c766b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_links_user_prompt('https://edwarddonner.com'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c302221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_links(url):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": link_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_links_user_prompt(url)}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    links = json.loads(result)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738f8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_relevant_links(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f856c",
   "metadata": {},
   "source": [
    "## Second step: make the brochure!\n",
    "\n",
    "Assemble all the details into another prompt to GPT-5-nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4005366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page_and_all_relevant_links(url):\n",
    "    contents = fetch_website_contents(url)\n",
    "    relevant_links = select_relevant_links(url)\n",
    "    result = f\"## Landing Page:\\n\\n{contents}\\n## Relevant Links:\\n\"\n",
    "    for link in relevant_links['links']:\n",
    "        result += f\"\\n\\n### Link: {link['type']}\\n\"\n",
    "        result += fetch_website_contents(link[\"url\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "724b681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(fetch_page_and_all_relevant_links(\"https://huggingface.co\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f63a38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "brochure_system_prompt = \"\"\"\n",
    "You are an assistant that analyzes the contents of several relevant pages from a company website\n",
    "and creates a short brochure about the company for prospective customers, investors and recruits.\n",
    "Respond in markdown without code blocks.\n",
    "Include details of company culture, customers and careers/jobs if you have the information.\n",
    "\"\"\"\n",
    "\n",
    "# Or uncomment the lines below for a more humorous brochure - this demonstrates how easy it is to incorporate 'tone':\n",
    "\n",
    "# brochure_system_prompt = \"\"\"\n",
    "# You are an assistant that analyzes the contents of several relevant pages from a company website\n",
    "# and creates a short, humorous, entertaining, witty brochure about the company for prospective customers, investors and recruits.\n",
    "# Respond in markdown without code blocks.\n",
    "# Include details of company culture, customers and careers/jobs if you have the information.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a7ae5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brochure_user_prompt(company_name, url):\n",
    "    user_prompt = f\"\"\"\n",
    "You are looking at a company called: {company_name}\n",
    "Here are the contents of its landing page and other relevant pages;\n",
    "use this information to build a short brochure of the company in markdown without code blocks.\\n\\n\n",
    "\"\"\"\n",
    "    user_prompt += fetch_page_and_all_relevant_links(url)\n",
    "    user_prompt = user_prompt[:5_000] # Truncate if more than 5,000 characters\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffc312d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_brochure_user_prompt(\"HuggingFace\", \"https://huggingface.co\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da7db4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brochure(company_name, url):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL, # or MODEL if you are using Ollama otherwise use gpt-4.1-mini or gpt-5-nano\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": brochure_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}\n",
    "        ],\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b63e4cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Hugging Face: The AI community building the future\n",
       "\n",
       "Hugging Face is a collaborative platform where the machine learning community creates, shares, and deploys models, datasets, and applications. With a strong open-source foundation, we help accelerate ML from research to real-world impact.\n",
       "\n",
       "---\n",
       "\n",
       "## What we offer\n",
       "\n",
       "- A thriving collaboration hub for unlimited public models, datasets, and applications.\n",
       "- Scale and speed with the HF open-source stack.\n",
       "- Access to a diverse, multi-modal ecosystem: text, image, video, audio, and even 3D.\n",
       "- Build and showcase your ML portfolio by sharing work with the global community.\n",
       "- Enterprise-grade options: paid compute and dedicated solutions for teams and organizations.\n",
       "\n",
       "- Global reach and growth: browse 1M+ models, 250k+ datasets, and 400k+ applications.\n",
       "- Explore prebuilt AI apps and spaces to run, test, and deploy quickly.\n",
       "\n",
       "---\n",
       "\n",
       "## Our platform components\n",
       "\n",
       "- Models: A vast collection of models across tasks and modalities (e.g., trending models like moonshotai/Kimi-K2-Thinking, maya-research/maya1, and many more).\n",
       "- Datasets: Large and diverse datasets (e.g., nvidia/PhysicalAI-Autonomous-Vehicles, fka/awesome-chatgpt-prompts, Open-Bee/Honey-Data-15M, and more).\n",
       "- Spaces: Hosted AI apps and demos you can run, customize, and extend (e.g., “The Smol Training Playbook,” image/video edit apps, DeepSite v3, and others).\n",
       "- Community: A vibrant network of researchers, engineers, and practitioners sharing tools, know-how, and成果.\n",
       "- Docs & Enterprise: Comprehensive documentation and enterprise-grade offerings to accelerate ML workflows.\n",
       "- Pricing & Sign Up: Flexible plans to fit individual creators, teams, and organizations.\n",
       "\n",
       "---\n",
       "\n",
       "## Our culture and community\n",
       "\n",
       "- The AI community building the future: open collaboration, transparency, and a shared drive to democratize AI.\n",
       "- Open-source first: leveraging a robust HF open-source stack to move faster and more inclusively.\n",
       "- All modalities and applications: tools and models spanning text, image, video, audio, and 3D.\n",
       "- A platform for you to shine: build, publish, and grow your ML profile by sharing your work with the world.\n",
       "\n",
       "---\n",
       "\n",
       "## Customers, use cases, and impact\n",
       "\n",
       "- Organizations and individuals leverage Hugging Face to develop, evaluate, and deploy ML models with ease.\n",
       "- Use cases span research, industry applications, and creative apps, from NLP to computer vision and multimodal projects.\n",
       "- Examples you can explore on the platform include training dashboards, model decks, image edit workflows, and novel AI applications built with Spaces.\n",
       "- We support teams with enterprise compute and services to accelerate ML projects at scale.\n",
       "\n",
       "---\n",
       "\n",
       "## Careers at Hugging Face\n",
       "\n",
       "- A growing team focused on ML, NLP, and deep learning, with opportunities to influence the AI landscape.\n",
       "- Company snapshot: founded in 2016, privately held, based in Paris, with a fast-growing global community (staff sizes in the range of a few dozen to a couple hundred).\n",
       "- Open roles and current openings are listed on our Careers page. If you’re excited about open science, collaboration, and building tools that democratize AI, Hugging Face could be a fit.\n",
       "- See jobs and learn more about joining our team via the Careers page: Hugging Face – Current Openings.\n",
       "\n",
       "---\n",
       "\n",
       "## Get started\n",
       "\n",
       "- Visit hugginface.co to explore models, datasets, spaces, and docs.\n",
       "- Sign up to join the community, publish your work, and access enterprise options when you’re ready to scale.\n",
       "- For career opportunities, check the Careers page for current openings.\n",
       "\n",
       "Hugging Face is the AI community building the future—join us to create, share, and deploy the next generation of AI together."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_brochure(\"HuggingFace\", \"https://huggingface.co\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b885e",
   "metadata": {},
   "source": [
    "## Finally - a minor improvement\n",
    "\n",
    "With a small adjustment, we can change this so that the results stream back from OpenAI,\n",
    "with the familiar typewriter animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02b83f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_brochure(company_name, url):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL, # or MODEL if you are using Ollama otherwise use gpt-4.1-mini or gpt-5-nano\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": brochure_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}\n",
    "          ],\n",
    "        stream=True\n",
    "    )    \n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "874eb7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Hugging Face: The AI community building the future\n",
       "\n",
       "Hugging Face is the platform where the machine learning community collaborates on models, datasets, and applications. We’re on a mission to solve and democratize artificial intelligence through natural language, and we’re powered by an open, collaborative spirit.\n",
       "\n",
       "## What we do\n",
       "- Create, discover, and collaborate on ML projects with the HF open source stack.\n",
       "- Host and share unlimited public models, datasets, and applications.\n",
       "- Support all modalities: text, image, video, audio, and even 3D.\n",
       "- Help organizations move faster with enterprise-grade compute and deployments.\n",
       "\n",
       "## Our Platform\n",
       "- Models: Browse 1M+ models to find components for your next project.\n",
       "- Datasets: Explore 250k+ datasets to train and evaluate your models.\n",
       "- Spaces: Run, deploy, and share AI apps with an easy-to-use infrastructure (examples include real-time inference, image and video editing apps, and more).\n",
       "- Community and Docs: Rich documentation and a global community to learn from and contribute to.\n",
       "- Enterprise & Pricing: Paid compute and enterprise solutions for teams and organizations.\n",
       "- Multimodal capabilities: Text, image, video, audio, and 3D workflows supported.\n",
       "\n",
       "## Why it matters for customers\n",
       "- Accelerate ML development: Access a vast ecosystem of models, datasets, and apps in one place.\n",
       "- Collaborate openly: Share work with the world, get feedback, and build your ML portfolio.\n",
       "- Scale with enterprise options: Compute resources and enterprise-grade solutions to accelerate production.\n",
       "\n",
       "## For developers, researchers and builders\n",
       "- Build your portfolio: Share your work publicly and showcase your ML profile.\n",
       "- Explore all modalities: Work across text, images, video, audio, and 3D.\n",
       "- Learn and contribute: Join a vibrant community that advances NLP, ML, and deep learning.\n",
       "\n",
       "## Culture and community\n",
       "- The AI community building the future: A collaborative, open, and inclusive culture that democratizes AI.\n",
       "- Open source first: Move faster by leveraging and contributing to an open source stack.\n",
       "- Global reach, Paris roots: Founded in 2016, Hugging Face is privately held with a base in Paris, France, growing a diverse team around the world.\n",
       "\n",
       "## Careers and opportunities\n",
       "- Current openings: Hugging Face publicly lists career opportunities; join a growing team shaping the future of AI.\n",
       "- Company profile: Private, 51–200 employees, founded in 2016; specialties in machine learning, natural language processing, and deep learning.\n",
       "- Get involved: Explore roles that align with research, engineering, product, and enterprise solutions.\n",
       "\n",
       "## Quick facts\n",
       "- Founded: 2016\n",
       "- Location: Paris, FR (primary)\n",
       "- Size: 51–200 employees\n",
       "- Industry: Software Development\n",
       "- Core mission: Solve and democratize artificial intelligence through natural language\n",
       "- Key products: Hugging Face platform, datasets, models, and Spaces\n",
       "- Notable capability: Browse 1M+ models, 250k+ datasets, and 400k+ applications\n",
       "\n",
       "Explore, build, and collaborate with Hugging Face—the home of machine learning. If you’re a customer, a contributor, or a future teammate, your work can help push AI forward."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try changing the system prompt to the humorous version when you make the Brochure for Hugging Face:\n",
    "\n",
    "stream_brochure(\"HuggingFace\", \"https://huggingface.co\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
